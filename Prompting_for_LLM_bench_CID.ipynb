{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72822190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5518, 16)\n",
      "     doc_id                                       passage_text entity1_id  \\\n",
      "0  10087562  Torsade de pointes ventricular tachycardia dur...    D004280   \n",
      "1  10087562  Torsade de pointes ventricular tachycardia dur...    D004280   \n",
      "2  10087562  Torsade de pointes ventricular tachycardia dur...    D004280   \n",
      "3  10087562  Torsade de pointes ventricular tachycardia dur...    D004280   \n",
      "4  10087562  Torsade de pointes ventricular tachycardia dur...    D004280   \n",
      "\n",
      "                   entity1_text entity1_type entity1_ann_id  entity1_offset  \\\n",
      "0  ['Dubutamine', 'dobutamine']     Chemical              2              72   \n",
      "1  ['Dubutamine', 'dobutamine']     Chemical              2              72   \n",
      "2  ['Dubutamine', 'dobutamine']     Chemical              2              72   \n",
      "3  ['Dubutamine', 'dobutamine']     Chemical              2              72   \n",
      "4  ['Dubutamine', 'dobutamine']     Chemical              2              72   \n",
      "\n",
      "  entity1_length entity2_id                                   entity2_text  \\\n",
      "0             10    D016171   ['Torsade de pointes', 'torsade de pointes']   \n",
      "1             10    D017180                    ['ventricular tachycardia']   \n",
      "2             10    D002311                     ['dilated cardiomyopathy']   \n",
      "3             10    D006333  ['congestive heart failure', 'heart failure']   \n",
      "4             10    D001145     ['arrhythmias', 'ventricular arrhythmias']   \n",
      "\n",
      "  entity2_type entity2_ann_id  entity2_offset entity2_length relation_type  \\\n",
      "0      Disease              0               0             18           CID   \n",
      "1      Disease              1              19             23   No_Relation   \n",
      "2      Disease              3             111             22   No_Relation   \n",
      "3      Disease              4             138             24   No_Relation   \n",
      "4      Disease              7             315             23   No_Relation   \n",
      "\n",
      "  is_novel  \n",
      "0  Unknown  \n",
      "1  Unknown  \n",
      "2  Unknown  \n",
      "3  Unknown  \n",
      "4  Unknown  \n",
      "(5585, 16)\n",
      "(5397, 19)\n",
      "     doc_id                                       passage_text entity1_id  \\\n",
      "0  10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
      "1  10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
      "2  10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
      "3  10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
      "4  10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
      "\n",
      "                  entity1_text entity1_type entity1_ann_id  entity1_offset  \\\n",
      "0  ['22-oxacalcitriol', 'OCT']     Chemical              0             0.0   \n",
      "1  ['22-oxacalcitriol', 'OCT']     Chemical              0             0.0   \n",
      "2  ['22-oxacalcitriol', 'OCT']     Chemical              0             0.0   \n",
      "3  ['22-oxacalcitriol', 'OCT']     Chemical              0             0.0   \n",
      "4  ['22-oxacalcitriol', 'OCT']     Chemical              0             0.0   \n",
      "\n",
      "  entity1_length entity2_id  \\\n",
      "0             16    D006962   \n",
      "1             16    D001851   \n",
      "2             16    D051437   \n",
      "3             16    D006934   \n",
      "4             16    D007674   \n",
      "\n",
      "                                        entity2_text entity2_type  \\\n",
      "0                  ['secondary hyperparathyroidism']      Disease   \n",
      "1  ['adynamic bone disease', 'low bone turnover',...      Disease   \n",
      "2           ['renal failure', 'renal insufficiency']      Disease   \n",
      "3                                  ['hypercalcemia']      Disease   \n",
      "4                        ['impaired renal function']      Disease   \n",
      "\n",
      "  entity2_ann_id  entity2_offset entity2_length relation_type is_novel  \\\n",
      "0              1            28.0             29   No_Relation  Unknown   \n",
      "1              2            75.0             17   No_Relation  Unknown   \n",
      "2              3           106.0             13   No_Relation  Unknown   \n",
      "3              6           273.0             13           CID  Unknown   \n",
      "4             13           591.0             23   No_Relation  Unknown   \n",
      "\n",
      "  filename count percentage  \n",
      "0      NaN   NaN        NaN  \n",
      "1      NaN   NaN        NaN  \n",
      "2      NaN   NaN        NaN  \n",
      "3      NaN   NaN        NaN  \n",
      "4      NaN   NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "def merge_csvs_to_df(folder):\n",
    "    import os\n",
    "    import csv\n",
    "    import pandas as pd\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                path = os.path.join(root, file)\n",
    "                with open(path, newline='', encoding='utf-8') as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    for row in reader:\n",
    "                        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Ensure correct types for sorting\n",
    "    df['doc_id'] = df['doc_id'].astype(str)\n",
    "    df['entity1_offset'] = pd.to_numeric(df['entity1_offset'])\n",
    "    df['entity2_offset'] = pd.to_numeric(df['entity2_offset'])\n",
    "    # Sort by doc_id, then offsets\n",
    "    df = df.sort_values(by=['doc_id', 'entity1_offset', 'entity2_offset']).reset_index(drop=True)\n",
    "    # Remove duplicate rows\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "folder = \"LLM_benchmarks/other_data/cdr/test\"\n",
    "merged_df_test = merge_csvs_to_df(folder)\n",
    "print(merged_df_test.shape)  # Print shape of the merged DataFrame\n",
    "print(merged_df_test.head()) # Print first few rows \n",
    "folder_train = \"LLM_benchmarks/other_data/cdr/train\"\n",
    "merged_df_train = merge_csvs_to_df(folder_train)\n",
    "print(merged_df_train.shape)  # Print shape of the merged DataFrame\n",
    "folder_dev = \"LLM_benchmarks/other_data/cdr/dev\"\n",
    "merged_df_dev = merge_csvs_to_df(folder_dev)\n",
    "print(merged_df_dev.shape)  # Print shape of the merged DataFrame\n",
    "print(merged_df_dev.head()) # Print first few rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5c30374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique relation types: ['CID' 'No_Relation']\n",
      "Number of unique relation types: 2\n",
      "Relation type counts test:\n",
      "relation_type\n",
      "No_Relation    4454\n",
      "CID            1064\n",
      "Name: count, dtype: int64\n",
      "Relation type counts train:\n",
      "relation_type\n",
      "No_Relation    4548\n",
      "CID            1037\n",
      "Name: count, dtype: int64\n",
      "Number of duplicate rows in the DataFrame: 0\n",
      "5518\n",
      "5585\n"
     ]
    }
   ],
   "source": [
    "unique_relations = merged_df_test['relation_type'].unique()\n",
    "print(\"Unique relation types:\", unique_relations)\n",
    "print(\"Number of unique relation types:\", len(unique_relations))\n",
    "relation_counts = merged_df_test['relation_type'].value_counts()\n",
    "relation_counts2 = merged_df_train['relation_type'].value_counts()\n",
    "print(\"Relation type counts test:\")\n",
    "print(relation_counts)\n",
    "print(\"Relation type counts train:\")\n",
    "print(relation_counts2)\n",
    "num_duplicates = merged_df_test.duplicated().sum()\n",
    "print(\"Number of duplicate rows in the DataFrame:\", num_duplicates)\n",
    "print(len(merged_df_test))\n",
    "print(len(merged_df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f6bb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_prompt_examples_single(df, n_prompts=3, seed=42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "\n",
    "    relation_types = df['relation_type'].unique()\n",
    "    prompts = []\n",
    "\n",
    "    for prompt_num in range(n_prompts):\n",
    "        used_indices = set()\n",
    "        examples = []\n",
    "        example_num = 1\n",
    "        random.seed(seed + prompt_num)\n",
    "        np.random.seed(seed + prompt_num)\n",
    "\n",
    "        for rel in relation_types:\n",
    "            rel_rows = df[df['relation_type'] == rel]\n",
    "            if rel_rows.empty:\n",
    "                continue\n",
    "            available_indices = list(set(rel_rows.index) - used_indices)\n",
    "            if available_indices:\n",
    "                idx = random.choice(available_indices)\n",
    "            else:\n",
    "                idx = random.choice(rel_rows.index)\n",
    "            used_indices.add(idx)\n",
    "            row = df.loc[idx]\n",
    "            entities = f\"Entity1: {row['entity1_text']} ({row['entity1_type']}), Entity2: {row['entity2_text']} ({row['entity2_type']})\"\n",
    "            example = (\n",
    "                f\"Example {example_num}\\n\"\n",
    "                f\"## Input:\\n\"\n",
    "                f\"Text: {row['passage_text']}\\n\"\n",
    "                f\"{entities}\\n\"\n",
    "                f\"## Output:\\n\"\n",
    "                f\"{entities}, Relation: {row['relation_type']}\\n\"\n",
    "            )\n",
    "            examples.append(example)\n",
    "            example_num += 1\n",
    "            if len(examples) >= 9:\n",
    "                break\n",
    "\n",
    "        prompt_text = \"\\n\".join(examples)\n",
    "        prompts.append(prompt_text)\n",
    "\n",
    "    return prompts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9882698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_prompt_examples_multi(df, n_prompts=1, seed=42, number_relations=True, relations_entitys_separately=False):\n",
    "    import random\n",
    "    import numpy as np\n",
    "\n",
    "    relation_types = df['relation_type'].unique()\n",
    "    prompts = []\n",
    "\n",
    "    for prompt_num in range(n_prompts):\n",
    "        used_docids = set()\n",
    "        examples = []\n",
    "        example_num = 1\n",
    "        random.seed(seed + prompt_num)\n",
    "        np.random.seed(seed + prompt_num)\n",
    "\n",
    "        for rel in relation_types:\n",
    "            rel_rows = df[df['relation_type'] == rel]\n",
    "            if rel_rows.empty:\n",
    "                continue\n",
    "            available_indices = list(set(rel_rows.index) - used_docids)\n",
    "            if available_indices:\n",
    "                idx = random.choice(available_indices)\n",
    "            else:\n",
    "                idx = random.choice(rel_rows.index)\n",
    "            doc_id = df.loc[idx, 'doc_id']\n",
    "            if doc_id in used_docids:\n",
    "                continue\n",
    "            used_docids.add(doc_id)\n",
    "            doc_rows = df[df['doc_id'] == doc_id]\n",
    "\n",
    "            passage_text = doc_rows.iloc[0]['passage_text']\n",
    "            # List all entity pairs\n",
    "            if number_relations:\n",
    "                entity_lines = [\n",
    "                    f\"{i+1}. Entity1: {row['entity1_text']} ({row['entity1_type']}), Entity2: {row['entity2_text']} ({row['entity2_type']})\"\n",
    "                    for i, (_, row) in enumerate(doc_rows.iterrows())\n",
    "                ]\n",
    "            else:\n",
    "                entity_lines = [\n",
    "                    f\"Entity1: {row['entity1_text']} ({row['entity1_type']}), Entity2: {row['entity2_text']} ({row['entity2_type']})\"\n",
    "                    for _, row in doc_rows.iterrows()\n",
    "                ]\n",
    "            if number_relations:\n",
    "                if relations_entitys_separately:\n",
    "                    relation_lines = [\n",
    "                        f\"{i+1}. Relation: {row['relation_type']}\"\n",
    "                        for i, (_, row) in enumerate(doc_rows.iterrows())\n",
    "                    ]\n",
    "                else:\n",
    "                    relation_lines = [\n",
    "                        f\"{i+1}. Entity1: {row['entity1_text']}, Entity2: {row['entity2_text']}, Relation: {row['relation_type']}\"\n",
    "                        for i, (_, row) in enumerate(doc_rows.iterrows())\n",
    "                    ]\n",
    "            else:\n",
    "                relation_lines = [\n",
    "                    f\"Entity1: {row['entity1_text']}, Entity2: {row['entity2_text']}, Relation: {row['relation_type']}\"\n",
    "                    for _, row in doc_rows.iterrows()\n",
    "                ]\n",
    "            example = (\n",
    "                f\"Example {example_num}\\n\"\n",
    "                #f\"DocID: {doc_id}\\n\"\n",
    "                f\"## Input:\\n\"\n",
    "                f\"Text: {passage_text}\\n\"\n",
    "                f\"Entities:\\n\" + \"\\n\".join(entity_lines) + \"\\n\"\n",
    "                f\"## Output:\\n\" + \"\\n\".join(relation_lines) + \"\\n\"\n",
    "            )\n",
    "            examples.append(example)\n",
    "            example_num += 1\n",
    "            if len(examples) >= 9:\n",
    "                break\n",
    "\n",
    "        prompt_text = \"\\n\".join(examples)\n",
    "        prompts.append(prompt_text)\n",
    "\n",
    "    return prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28b1bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_prompt(template_path, text, e1=\"\", entity_type1=\"\", e2=\"\", entity_type2=\"\", examples=\"\", entity_list=\"\", prompt_id=None):\n",
    "    with open(template_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        template = f.read()\n",
    "    filled = template.format(\n",
    "        text=text,\n",
    "        e1=e1,\n",
    "        entity_type1=entity_type1,\n",
    "        e2=e2,\n",
    "        entity_type2=entity_type2,\n",
    "        examples=examples,\n",
    "        entity_list=entity_list,\n",
    "        prompt_id=prompt_id\n",
    "    )\n",
    "    return filled\n",
    "\n",
    "\n",
    "def create_prompt_relation_df_single(df,df_2, template_path, examples=False,same_examples=False, seed=42, n_prompts=1, prompt_id=False):\n",
    "    import pandas as pd\n",
    "    prompts = []\n",
    "    relations = []\n",
    "    doc_ids = []\n",
    "    runs = []\n",
    "    prompt_ids = []\n",
    "\n",
    "    prompt_id_counter = 1  # Start numbering from 1\n",
    "\n",
    "    for run_idx in range(1, n_prompts + 1):\n",
    "        if examples:\n",
    "            if same_examples == True:\n",
    "                example_prompts = generate_multiple_prompt_examples_single(df_2, n_prompts=1, seed=seed)\n",
    "                examples_str = example_prompts[0] if example_prompts else \"\"\n",
    "            else:\n",
    "            # Generate new examples for this run with updated seed\n",
    "                example_prompts = generate_multiple_prompt_examples_single(df_2, n_prompts=1, seed=seed + run_idx - 1)\n",
    "            # If you want to use these as context/examples, you can pass them to fill_prompt as needed\n",
    "            # Here, we assume you want to add them as an 'examples' variable in the template\n",
    "                examples_str = example_prompts[0] if example_prompts else \"\"\n",
    "        else:\n",
    "            examples_str = \"\"\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            if prompt_id:\n",
    "                prompt = fill_prompt(\n",
    "                    template_path,\n",
    "                    text=row['passage_text'],\n",
    "                    e1=row['entity1_text'],\n",
    "                    entity_type1=row['entity1_type'],\n",
    "                    e2=row['entity2_text'],\n",
    "                    entity_type2=row['entity2_type'],\n",
    "                    examples=examples_str,\n",
    "                    prompt_id=prompt_id_counter\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "                relations.append(row['relation_type'])\n",
    "                doc_ids.append(row['doc_id'])\n",
    "                runs.append(run_idx)\n",
    "                prompt_ids.append(prompt_id_counter)\n",
    "                prompt_id_counter += 1\n",
    "            else:\n",
    "                prompt = fill_prompt(\n",
    "                    template_path,\n",
    "                    text=row['passage_text'],\n",
    "                    e1=row['entity1_text'],\n",
    "                    entity_type1=row['entity1_type'],\n",
    "                    e2=row['entity2_text'],\n",
    "                    entity_type2=row['entity2_type'],\n",
    "                    examples=examples_str,\n",
    "                    prompt_id=\"\"\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "                relations.append(row['relation_type'])\n",
    "                doc_ids.append(row['doc_id'])\n",
    "                runs.append(run_idx)\n",
    "                prompt_ids.append(prompt_id_counter)\n",
    "                prompt_id_counter += 1\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'Prompt': prompts,\n",
    "        'Relation': relations,\n",
    "        'DocID': doc_ids,\n",
    "        'run': runs,\n",
    "        'prompt_id': prompt_ids\n",
    "    })\n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prompts_df_to_json_from_runs(df, n_key=\"n_1\"):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame with 'Prompt', 'Relation', 'DocID', and 'run' columns to the required JSON structure.\n",
    "    Each run contains lists for prompts, relations, and doc_ids.\n",
    "    \"\"\"\n",
    "    runs = {}\n",
    "    for run_num in sorted(df['run'].unique()):\n",
    "        run_df = df[df['run'] == run_num]\n",
    "        prompts_list = run_df['Prompt'].tolist()\n",
    "        relations_list = run_df['Relation'].tolist()\n",
    "        doc_ids_list = run_df['DocID'].tolist()\n",
    "        runs[f\"run_{run_num}\"] = {\n",
    "            \"prompts1\": prompts_list,\n",
    "            \"relations\": relations_list,\n",
    "            \"doc_ids\": doc_ids_list\n",
    "        }\n",
    "\n",
    "    result = {n_key: runs}\n",
    "    return result\n",
    "\n",
    "def prompts_df_to_json_from_runs_multi(df, n_key=\"n_1\"):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame with 'Prompt', 'Relation'/'Multi_Relations', 'DocID'/'Multi_DocID', and 'run' columns\n",
    "    to the required JSON structure. Each run contains lists for prompts, relations, and doc_ids.\n",
    "    The JSON keys will match the DataFrame column names (e.g., 'Multi_Relations' -> 'multi_relations').\n",
    "    \"\"\"\n",
    "    runs = {}\n",
    "    # Find the correct column names for relations and doc_ids\n",
    "    rel_col = None\n",
    "    docid_col = None\n",
    "    for c in df.columns:\n",
    "        if c.lower().startswith(\"relation\"):\n",
    "            rel_col = c\n",
    "        if c.lower().startswith(\"multi_relation\"):\n",
    "            rel_col = c\n",
    "        if c.lower() == \"relations\":\n",
    "            rel_col = c\n",
    "        if c.lower() == \"multi_relations\":\n",
    "            rel_col = c\n",
    "        if c.lower().startswith(\"docid\"):\n",
    "            docid_col = c\n",
    "        if c.lower().startswith(\"multi_docid\"):\n",
    "            docid_col = c\n",
    "        if c.lower() == \"doc_ids\":\n",
    "            docid_col = c\n",
    "        if c.lower() == \"multi_docid\":\n",
    "            docid_col = c\n",
    "\n",
    "    # Fallbacks if not found\n",
    "    if rel_col is None:\n",
    "        rel_col = \"Relation\"\n",
    "    if docid_col is None:\n",
    "        docid_col = \"DocID\"\n",
    "\n",
    "    # Use lower-case JSON keys for consistency\n",
    "    rel_json_key = rel_col.lower()\n",
    "    docid_json_key = docid_col.lower()\n",
    "\n",
    "    for run_num in sorted(df['run'].unique()):\n",
    "        run_df = df[df['run'] == run_num]\n",
    "        prompts_list = run_df['Prompt'].tolist()\n",
    "        relations_list = run_df[rel_col].tolist()\n",
    "        doc_ids_list = run_df[docid_col].tolist()\n",
    "        runs[f\"run_{run_num}\"] = {\n",
    "            \"prompts1\": prompts_list,\n",
    "            rel_json_key: relations_list,\n",
    "            docid_json_key: doc_ids_list\n",
    "        }\n",
    "\n",
    "    result = {n_key: runs}\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "#prompt_json = prompts_df_to_json_from_runs(prompt_relation_df)\n",
    "#with open(\"prompts_for_api.json\", \"w\") as f:\n",
    "#    json.dump(prompt_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7824a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_prompt_relation_df_multi_by_docid(\n",
    "    df, df_2, template_path, examples=False, same_examples=False,\n",
    "    number_relations=False, relations_entitys_separately=False,\n",
    "    seed=42, n_prompts=1, shuffle=False, new_shuffle_each_run = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Create prompts for multi-relation extraction grouped by doc_id.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with entity pairs to create prompts for\n",
    "        df_2: DataFrame to use for generating few-shot examples\n",
    "        template_path: Path to the prompt template file\n",
    "        examples: Whether to include few-shot examples\n",
    "        same_examples: Whether to use the same examples across all runs\n",
    "        number_relations: Whether to number the entity pairs in the prompt\n",
    "        relations_entitys_separately: Whether to show relations separately from entities\n",
    "        seed: Random seed for reproducibility\n",
    "        n_prompts: Number of runs to generate\n",
    "        shuffle: If True, shuffle entity pairs within each doc_id for each run.\n",
    "                 Different runs will have different shuffle orders for the same doc_id.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: Prompt, Multi_Relations, Multi_DocID, run\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import random\n",
    "\n",
    "    prompts = []\n",
    "    relations = []\n",
    "    doc_ids = []\n",
    "    runs = []\n",
    "    shuffle_seed = seed\n",
    "    # Group by doc_id\n",
    "    grouped = df.groupby('doc_id')\n",
    "\n",
    "    for run_idx in range(1, n_prompts + 1):\n",
    "        if examples:\n",
    "            if same_examples:\n",
    "                example_prompts = generate_multiple_prompt_examples_multi(\n",
    "                    df_2, n_prompts=1, seed=seed,\n",
    "                    number_relations=number_relations,\n",
    "                    relations_entitys_separately=relations_entitys_separately\n",
    "                )\n",
    "                examples_str = example_prompts[0] if example_prompts else \"\"\n",
    "            else:\n",
    "                example_prompts = generate_multiple_prompt_examples_multi(\n",
    "                    df_2, n_prompts=1, seed=seed + run_idx - 1,\n",
    "                    number_relations=number_relations,\n",
    "                    relations_entitys_separately=relations_entitys_separately\n",
    "                )\n",
    "                examples_str = example_prompts[0] if example_prompts else \"\"\n",
    "        else:\n",
    "            examples_str = \"\"\n",
    "\n",
    "        for doc_id, doc_rows in grouped:\n",
    "            # If shuffle is enabled, shuffle the rows for this doc_id\n",
    "            # Use a seed based on: base_seed + run_idx + hash(doc_id) for reproducibility\n",
    "            # and different shuffle per run for the same doc_id\n",
    "            if shuffle:\n",
    "                if new_shuffle_each_run == True: ## check if we want to shuffle for each run \n",
    "                    shuffle_seed = seed + run_idx + hash(str(doc_id)) % 10000\n",
    "                else:\n",
    "                    shuffle_seed = seed\n",
    "\n",
    "                random.seed(shuffle_seed)\n",
    "                # Shuffle the indices and reorder doc_rows\n",
    "                shuffled_indices = list(range(len(doc_rows)))\n",
    "                random.shuffle(shuffled_indices)\n",
    "                doc_rows = doc_rows.iloc[shuffled_indices].reset_index(drop=True)\n",
    "\n",
    "            # Build entity_lines for this doc_id\n",
    "            if number_relations:\n",
    "                entity_lines = [\n",
    "                    f\"{i+1}. Entity1: {row['entity1_text']} ({row['entity1_type']}), Entity2: {row['entity2_text']} ({row['entity2_type']})\"\n",
    "                    for i, (_, row) in enumerate(doc_rows.iterrows())\n",
    "                ]\n",
    "            else:\n",
    "                entity_lines = [\n",
    "                    f\"Entity1: {row['entity1_text']} ({row['entity1_type']}), Entity2: {row['entity2_text']} ({row['entity2_type']})\"\n",
    "                    for _, row in doc_rows.iterrows()\n",
    "                ]\n",
    "            # Collect relations for this doc_id (in the same order as entity_lines)\n",
    "            relation_list = doc_rows['relation_type'].tolist()\n",
    "\n",
    "            # Use the first passage_text for the prompt\n",
    "            passage_text = doc_rows.iloc[0]['passage_text']\n",
    "\n",
    "            prompt = fill_prompt(\n",
    "                template_path,\n",
    "                text=passage_text,\n",
    "                entity_list=\"\\n\".join(entity_lines),\n",
    "                examples=examples_str\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "            relations.append(relation_list)\n",
    "            doc_ids.append(doc_id)\n",
    "            runs.append(run_idx)\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'Prompt': prompts,\n",
    "        'Multi_Relations': relations,\n",
    "        'Multi_DocID': doc_ids,\n",
    "        'run': runs\n",
    "    })\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beab719",
   "metadata": {},
   "source": [
    "# Alternative function or shuffle for even split of realtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19765d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_362665/777292713.py:147: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df_cut = df_cut.groupby('doc_id').apply(balance_group).reset_index(drop=True)\n",
      "/tmp/ipykernel_362665/777292713.py:150: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df = df.groupby('doc_id').apply(balance_group).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation_type        CID  No_Relation\n",
      "row_index                            \n",
      "0              37.076023    62.923977\n",
      "1              31.812865    68.187135\n",
      "2              29.356725    70.643275\n",
      "3              30.681818    69.318182\n",
      "4              32.386364    67.613636\n",
      "5              33.806818    66.193182\n",
      "6              34.814815    65.185185\n",
      "7              44.444444    55.555556\n",
      "8              37.037037    62.962963\n",
      "9              31.578947    68.421053\n",
      "10             28.947368    71.052632\n",
      "11             35.526316    64.473684\n",
      "12             47.058824    52.941176\n",
      "13             23.529412    76.470588\n",
      "14             44.117647    55.882353\n",
      "15             50.000000    50.000000\n",
      "16             42.857143    57.142857\n",
      "17             21.428571    78.571429\n",
      "    Count  Percentage        CID  No_Relation\n",
      "3     503   58.830409  33.333333    66.666667\n",
      "6     217   25.380117  33.333333    66.666667\n",
      "9      59    6.900585  33.333333    66.666667\n",
      "12     42    4.912281  33.333333    66.666667\n",
      "15     20    2.339181  33.333333    66.666667\n",
      "18     14    1.637427  33.333333    66.666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_362665/777292713.py:157: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  shuffled_df = balanced_df_cut.groupby('doc_id').apply(lambda x: x.sample(frac=1, random_state=np.random.randint(1, 100))).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def print_summary_df(df):\n",
    "\n",
    "        # Step 1: Group by 'doc_id' and get the size of each group\n",
    "        group_sizes = df.groupby('doc_id').size()\n",
    "        size_counts = group_sizes.value_counts().sort_index()\n",
    "        total_groups = len(group_sizes)\n",
    "        size_percentages = (size_counts / total_groups) * 100\n",
    "\n",
    "        # Create a DataFrame for group sizes\n",
    "        group_sizes_df = pd.DataFrame({\n",
    "        'Number of Rows': size_counts.index,\n",
    "        'Count': size_counts.values,\n",
    "        'Percentage': size_percentages.values\n",
    "        }).set_index('Number of Rows')\n",
    "\n",
    "        # Step 2: Calculate label distribution for each group length\n",
    "        doc_rows = df.merge(\n",
    "        group_sizes.reset_index(name='group_length'),\n",
    "        on='doc_id'\n",
    "        )\n",
    "        label_distribution = (\n",
    "        doc_rows\n",
    "        .groupby(['group_length', 'relation_type'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        )\n",
    "        label_percentages = label_distribution.div(label_distribution.sum(axis=1), axis=0) * 100\n",
    "\n",
    "        # Step 3: Merge both results\n",
    "        result = pd.concat([group_sizes_df, label_percentages], axis=1)\n",
    "\n",
    "        print(result)\n",
    "\n",
    "def balance_group(group):\n",
    "    cid = group[group['relation_type'] == 'CID']\n",
    "    no_rel = group[group['relation_type'] == 'No_Relation']\n",
    "\n",
    "    # Calculate the desired number of rows for each label\n",
    "    min_len = min(len(cid), len(no_rel) // 2)  # Ensure at least 1:2 ratio\n",
    "    desired_cid = min_len\n",
    "    desired_no_rel = 2 * min_len\n",
    "\n",
    "    # Randomly sample the desired number of rows\n",
    "    balanced_cid = cid.sample(n=desired_cid, random_state=42, replace=False)\n",
    "    balanced_no_rel = no_rel.sample(n=desired_no_rel, random_state=42, replace=False)\n",
    "\n",
    "    # Combine and return\n",
    "    return pd.concat([balanced_cid, balanced_no_rel])\n",
    "\"\"\"\n",
    "def balance_group(group):\n",
    "    cid = group[group['relation_type'] == 'CID']\n",
    "    no_rel = group[group['relation_type'] == 'No_Relation']\n",
    "\n",
    "    # Calculate the desired number of rows for each label (50/50)\n",
    "    min_len = min(len(cid), len(no_rel))  # Ensure equal number of rows\n",
    "\n",
    "    # Randomly sample the desired number of rows\n",
    "    balanced_cid = cid.sample(n=min_len, random_state=42, replace=False)\n",
    "    balanced_no_rel = no_rel.sample(n=min_len, random_state=42, replace=False)\n",
    "\n",
    "    # Combine and return\n",
    "    return pd.concat([balanced_cid, balanced_no_rel])\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "def show_index_table(df):\n",
    "# Step 1: Assign an index to each row within its group\n",
    "        df['row_index'] = df.groupby('doc_id').cumcount()\n",
    "\n",
    "        # Step 2: For each index position, count the labels\n",
    "        label_distribution = (\n",
    "        df\n",
    "        .groupby(['row_index', 'relation_type'])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        )\n",
    "        # Step 3: Calculate the percentage distribution for each index position\n",
    "        label_percentages = label_distribution.div(label_distribution.sum(axis=1), axis=0) * 100\n",
    "\n",
    "        print(label_percentages)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def filter_and_truncate_groups(df, max, min):\n",
    "    # Step 1: Calculate the size of each group\n",
    "    group_sizes = df.groupby('doc_id').size()\n",
    "\n",
    "    # Step 2: Filter out groups with fewer than x rows\n",
    "    #valid_groups = group_sizes[group_sizes >= min].index\n",
    "    #filtered_df = df[df['doc_id'].isin(valid_groups)]\n",
    "\n",
    "    # Step 3: Truncate groups with more than x rows to exactly x rows\n",
    "    truncated_df = df.groupby('doc_id').head(max)\n",
    "\n",
    "    return truncated_df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def balance_group_with_margin(group, target_pct_cid=0.33, target_pct_no_rel=0.66, margin=0):\n",
    "    cid = group[group['relation_type'] == 'CID']\n",
    "    no_rel = group[group['relation_type'] == 'No_Relation']\n",
    "\n",
    "    len_cid = len(cid)\n",
    "    len_no_rel = len(no_rel)\n",
    "    total = len_cid + len_no_rel\n",
    "\n",
    "    # Calculate current percentages\n",
    "    current_pct_cid = len_cid / total\n",
    "    current_pct_no_rel = len_no_rel / total\n",
    "\n",
    "    # Check if current proportions are within the margin\n",
    "    if (abs(current_pct_cid - target_pct_cid) <= margin and\n",
    "        abs(current_pct_no_rel - target_pct_no_rel) <= margin):\n",
    "        return group  # Keep all rows if within margin\n",
    "\n",
    "    # If not, sample to get as close as possible to the target\n",
    "    desired_cid = round(total * target_pct_cid)\n",
    "    desired_no_rel = total - desired_cid\n",
    "\n",
    "    # Ensure we don't sample more than available\n",
    "    desired_cid = min(desired_cid, len_cid)\n",
    "    desired_no_rel = min(desired_no_rel, len_no_rel)\n",
    "\n",
    "    # Sample\n",
    "    balanced_cid = cid.sample(n=desired_cid, random_state=42, replace=False)\n",
    "    balanced_no_rel = no_rel.sample(n=desired_no_rel, random_state=42, replace=False)\n",
    "\n",
    "    return pd.concat([balanced_cid, balanced_no_rel])\n",
    "\n",
    "\n",
    "# Assuming df1 is the DataFrame with fewer columns,\n",
    "# and df2 is the DataFrame with 3 extra columns.\n",
    "columns_to_keep = merged_df_train.columns  # Columns present in df1\n",
    "merged_df_dev = merged_df_dev[columns_to_keep]     # Drop extra columns in df2\n",
    "df = pd.concat([merged_df_train, merged_df_dev], ignore_index=True)\n",
    "\n",
    "max_c = 18  # Your target number of rows per group\n",
    "min_c = 6\n",
    "#df = merged_df_train\n",
    "grouped = df.groupby('doc_id')\n",
    "df_cut = filter_and_truncate_groups(df, max=max_c, min=min_c)\n",
    "\n",
    "# balance data\n",
    "## balance data after cut\n",
    "balanced_df_cut = df_cut.groupby('doc_id').apply(balance_group).reset_index(drop=True)\n",
    "balanced_df_cut = balanced_df_cut.groupby('doc_id').filter(lambda x: len(x) > 0)\n",
    "#balance data before cut\n",
    "balanced_df = df.groupby('doc_id').apply(balance_group).reset_index(drop=True)\n",
    "#balanced_df = balanced_df.groupby('doc_id').filter(lambda x: len(x) > 0)\n",
    "balanced_df = filter_and_truncate_groups(balanced_df, max=max_c, min=min_c)\n",
    "\n",
    "#print_summary_df(balanced_df)\n",
    "\n",
    "# Step 1: Shuffle rows within each group\n",
    "shuffled_df = balanced_df_cut.groupby('doc_id').apply(lambda x: x.sample(frac=1, random_state=np.random.randint(1, 100))).reset_index(drop=True)\n",
    "\n",
    "# Step 2: Reassign row_index after shuffling\n",
    "shuffled_df['row_index'] = shuffled_df.groupby('doc_id').cumcount()\n",
    "\n",
    "\n",
    "show_index_table(shuffled_df)\n",
    "print_summary_df(shuffled_df)\n",
    "#show_index_table(df)\n",
    "\n",
    "\n",
    "#for doc_id, doc_rows in grouped:\n",
    "        #print(doc_id)\n",
    "        #print(doc_rows[\"relation_type\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cabda6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation_type        CID  No_Relation\n",
      "row_index                            \n",
      "0              32.982456    67.017544\n",
      "1              32.982456    67.017544\n",
      "2              33.099415    66.900585\n",
      "3              32.954545    67.045455\n",
      "4              32.954545    67.045455\n",
      "5              33.238636    66.761364\n",
      "6              33.333333    66.666667\n",
      "7              33.333333    66.666667\n",
      "8              35.555556    64.444444\n",
      "9              34.210526    65.789474\n",
      "10             35.526316    64.473684\n",
      "11             36.842105    63.157895\n",
      "12             35.294118    64.705882\n",
      "13             35.294118    64.705882\n",
      "14             35.294118    64.705882\n",
      "15             28.571429    71.428571\n",
      "16             28.571429    71.428571\n",
      "17             50.000000    50.000000\n",
      "    Count  Percentage        CID  No_Relation\n",
      "3     503   58.830409  33.333333    66.666667\n",
      "6     217   25.380117  33.333333    66.666667\n",
      "9      59    6.900585  33.333333    66.666667\n",
      "12     42    4.912281  33.333333    66.666667\n",
      "15     20    2.339181  33.333333    66.666667\n",
      "18     14    1.637427  33.333333    66.666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>passage_text</th>\n",
       "      <th>entity1_id</th>\n",
       "      <th>entity1_text</th>\n",
       "      <th>entity1_type</th>\n",
       "      <th>entity1_ann_id</th>\n",
       "      <th>entity1_offset</th>\n",
       "      <th>entity1_length</th>\n",
       "      <th>entity2_id</th>\n",
       "      <th>entity2_text</th>\n",
       "      <th>entity2_type</th>\n",
       "      <th>entity2_ann_id</th>\n",
       "      <th>entity2_offset</th>\n",
       "      <th>entity2_length</th>\n",
       "      <th>relation_type</th>\n",
       "      <th>is_novel</th>\n",
       "      <th>row_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>D002117</td>\n",
       "      <td>['Calcitriol']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>4</td>\n",
       "      <td>133.0</td>\n",
       "      <td>10</td>\n",
       "      <td>D005355</td>\n",
       "      <td>['fibrosis']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>28</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>8</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>D014807</td>\n",
       "      <td>['vitamin D']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>9</td>\n",
       "      <td>378.0</td>\n",
       "      <td>9</td>\n",
       "      <td>D001851</td>\n",
       "      <td>['adynamic bone disease', 'low bone turnover',...</td>\n",
       "      <td>Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>75.0</td>\n",
       "      <td>17</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>C051883</td>\n",
       "      <td>['22-oxacalcitriol', 'OCT']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>D006962</td>\n",
       "      <td>['secondary hyperparathyroidism']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>D014807</td>\n",
       "      <td>['vitamin D']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>9</td>\n",
       "      <td>378.0</td>\n",
       "      <td>9</td>\n",
       "      <td>D051437</td>\n",
       "      <td>['renal failure', 'renal insufficiency']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>3</td>\n",
       "      <td>106.0</td>\n",
       "      <td>13</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>D002117</td>\n",
       "      <td>['Calcitriol']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>4</td>\n",
       "      <td>133.0</td>\n",
       "      <td>10</td>\n",
       "      <td>D006962</td>\n",
       "      <td>['secondary hyperparathyroidism']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>D002117</td>\n",
       "      <td>['Calcitriol']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>4</td>\n",
       "      <td>133.0</td>\n",
       "      <td>10</td>\n",
       "      <td>D054559</td>\n",
       "      <td>['hyperphosphatemia']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>24</td>\n",
       "      <td>1721.0</td>\n",
       "      <td>17</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>C051883</td>\n",
       "      <td>['22-oxacalcitriol', 'OCT']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>D051437</td>\n",
       "      <td>['renal failure', 'renal insufficiency']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>3</td>\n",
       "      <td>106.0</td>\n",
       "      <td>13</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>C051883</td>\n",
       "      <td>['22-oxacalcitriol', 'OCT']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>D001851</td>\n",
       "      <td>['adynamic bone disease', 'low bone turnover',...</td>\n",
       "      <td>Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>75.0</td>\n",
       "      <td>17</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>C051883</td>\n",
       "      <td>['22-oxacalcitriol', 'OCT']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>D054559</td>\n",
       "      <td>['hyperphosphatemia']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>24</td>\n",
       "      <td>1721.0</td>\n",
       "      <td>17</td>\n",
       "      <td>CID</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>D002117</td>\n",
       "      <td>['Calcitriol']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>4</td>\n",
       "      <td>133.0</td>\n",
       "      <td>10</td>\n",
       "      <td>D006934</td>\n",
       "      <td>['hypercalcemia']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>6</td>\n",
       "      <td>273.0</td>\n",
       "      <td>13</td>\n",
       "      <td>CID</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>C051883</td>\n",
       "      <td>['22-oxacalcitriol', 'OCT']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>D006934</td>\n",
       "      <td>['hypercalcemia']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>6</td>\n",
       "      <td>273.0</td>\n",
       "      <td>13</td>\n",
       "      <td>CID</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10027919</td>\n",
       "      <td>22-oxacalcitriol suppresses secondary hyperpar...</td>\n",
       "      <td>D002117</td>\n",
       "      <td>['Calcitriol']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>4</td>\n",
       "      <td>133.0</td>\n",
       "      <td>10</td>\n",
       "      <td>D001851</td>\n",
       "      <td>['adynamic bone disease', 'low bone turnover',...</td>\n",
       "      <td>Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>75.0</td>\n",
       "      <td>17</td>\n",
       "      <td>CID</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10074612</td>\n",
       "      <td>Hypotension, bradycardia, and asystole after h...</td>\n",
       "      <td>D008775</td>\n",
       "      <td>['IVMP', 'methylprednisolone']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18</td>\n",
       "      <td>D007022</td>\n",
       "      <td>['Hypotension', 'hypotension']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>CID</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10074612</td>\n",
       "      <td>Hypotension, bradycardia, and asystole after h...</td>\n",
       "      <td>D008775</td>\n",
       "      <td>['IVMP', 'methylprednisolone']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18</td>\n",
       "      <td>D001919</td>\n",
       "      <td>['bradycardia']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11</td>\n",
       "      <td>CID</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10074612</td>\n",
       "      <td>Hypotension, bradycardia, and asystole after h...</td>\n",
       "      <td>D008775</td>\n",
       "      <td>['IVMP', 'methylprednisolone']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18</td>\n",
       "      <td>D006323</td>\n",
       "      <td>['asystole']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8</td>\n",
       "      <td>CID</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10074612</td>\n",
       "      <td>Hypotension, bradycardia, and asystole after h...</td>\n",
       "      <td>D008775</td>\n",
       "      <td>['IVMP', 'methylprednisolone']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18</td>\n",
       "      <td>D006331</td>\n",
       "      <td>['cardiac disease']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>9</td>\n",
       "      <td>367.0</td>\n",
       "      <td>15</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10074612</td>\n",
       "      <td>Hypotension, bradycardia, and asystole after h...</td>\n",
       "      <td>D008775</td>\n",
       "      <td>['IVMP', 'methylprednisolone']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18</td>\n",
       "      <td>D000860</td>\n",
       "      <td>['hypoxemia']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>13</td>\n",
       "      <td>513.0</td>\n",
       "      <td>9</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10074612</td>\n",
       "      <td>Hypotension, bradycardia, and asystole after h...</td>\n",
       "      <td>D008775</td>\n",
       "      <td>['IVMP', 'methylprednisolone']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18</td>\n",
       "      <td>D007511</td>\n",
       "      <td>['ischemic']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>8</td>\n",
       "      <td>358.0</td>\n",
       "      <td>8</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10074612</td>\n",
       "      <td>Hypotension, bradycardia, and asystole after h...</td>\n",
       "      <td>D008775</td>\n",
       "      <td>['IVMP', 'methylprednisolone']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18</td>\n",
       "      <td>D003645</td>\n",
       "      <td>['sudden death']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>15</td>\n",
       "      <td>803.0</td>\n",
       "      <td>12</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10074612</td>\n",
       "      <td>Hypotension, bradycardia, and asystole after h...</td>\n",
       "      <td>D008775</td>\n",
       "      <td>['IVMP', 'methylprednisolone']</td>\n",
       "      <td>Chemical</td>\n",
       "      <td>3</td>\n",
       "      <td>67.0</td>\n",
       "      <td>18</td>\n",
       "      <td>C538458</td>\n",
       "      <td>['pulmonary-renal syndrome']</td>\n",
       "      <td>Disease</td>\n",
       "      <td>10</td>\n",
       "      <td>432.0</td>\n",
       "      <td>24</td>\n",
       "      <td>No_Relation</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      doc_id                                       passage_text entity1_id  \\\n",
       "0   10027919  22-oxacalcitriol suppresses secondary hyperpar...    D002117   \n",
       "1   10027919  22-oxacalcitriol suppresses secondary hyperpar...    D014807   \n",
       "2   10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
       "3   10027919  22-oxacalcitriol suppresses secondary hyperpar...    D014807   \n",
       "4   10027919  22-oxacalcitriol suppresses secondary hyperpar...    D002117   \n",
       "5   10027919  22-oxacalcitriol suppresses secondary hyperpar...    D002117   \n",
       "6   10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
       "7   10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
       "8   10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
       "9   10027919  22-oxacalcitriol suppresses secondary hyperpar...    D002117   \n",
       "10  10027919  22-oxacalcitriol suppresses secondary hyperpar...    C051883   \n",
       "11  10027919  22-oxacalcitriol suppresses secondary hyperpar...    D002117   \n",
       "12  10074612  Hypotension, bradycardia, and asystole after h...    D008775   \n",
       "13  10074612  Hypotension, bradycardia, and asystole after h...    D008775   \n",
       "14  10074612  Hypotension, bradycardia, and asystole after h...    D008775   \n",
       "15  10074612  Hypotension, bradycardia, and asystole after h...    D008775   \n",
       "16  10074612  Hypotension, bradycardia, and asystole after h...    D008775   \n",
       "17  10074612  Hypotension, bradycardia, and asystole after h...    D008775   \n",
       "18  10074612  Hypotension, bradycardia, and asystole after h...    D008775   \n",
       "19  10074612  Hypotension, bradycardia, and asystole after h...    D008775   \n",
       "\n",
       "                      entity1_text entity1_type entity1_ann_id  \\\n",
       "0                   ['Calcitriol']     Chemical              4   \n",
       "1                    ['vitamin D']     Chemical              9   \n",
       "2      ['22-oxacalcitriol', 'OCT']     Chemical              0   \n",
       "3                    ['vitamin D']     Chemical              9   \n",
       "4                   ['Calcitriol']     Chemical              4   \n",
       "5                   ['Calcitriol']     Chemical              4   \n",
       "6      ['22-oxacalcitriol', 'OCT']     Chemical              0   \n",
       "7      ['22-oxacalcitriol', 'OCT']     Chemical              0   \n",
       "8      ['22-oxacalcitriol', 'OCT']     Chemical              0   \n",
       "9                   ['Calcitriol']     Chemical              4   \n",
       "10     ['22-oxacalcitriol', 'OCT']     Chemical              0   \n",
       "11                  ['Calcitriol']     Chemical              4   \n",
       "12  ['IVMP', 'methylprednisolone']     Chemical              3   \n",
       "13  ['IVMP', 'methylprednisolone']     Chemical              3   \n",
       "14  ['IVMP', 'methylprednisolone']     Chemical              3   \n",
       "15  ['IVMP', 'methylprednisolone']     Chemical              3   \n",
       "16  ['IVMP', 'methylprednisolone']     Chemical              3   \n",
       "17  ['IVMP', 'methylprednisolone']     Chemical              3   \n",
       "18  ['IVMP', 'methylprednisolone']     Chemical              3   \n",
       "19  ['IVMP', 'methylprednisolone']     Chemical              3   \n",
       "\n",
       "    entity1_offset entity1_length entity2_id  \\\n",
       "0            133.0             10    D005355   \n",
       "1            378.0              9    D001851   \n",
       "2              0.0             16    D006962   \n",
       "3            378.0              9    D051437   \n",
       "4            133.0             10    D006962   \n",
       "5            133.0             10    D054559   \n",
       "6              0.0             16    D051437   \n",
       "7              0.0             16    D001851   \n",
       "8              0.0             16    D054559   \n",
       "9            133.0             10    D006934   \n",
       "10             0.0             16    D006934   \n",
       "11           133.0             10    D001851   \n",
       "12            67.0             18    D007022   \n",
       "13            67.0             18    D001919   \n",
       "14            67.0             18    D006323   \n",
       "15            67.0             18    D006331   \n",
       "16            67.0             18    D000860   \n",
       "17            67.0             18    D007511   \n",
       "18            67.0             18    D003645   \n",
       "19            67.0             18    C538458   \n",
       "\n",
       "                                         entity2_text entity2_type  \\\n",
       "0                                        ['fibrosis']      Disease   \n",
       "1   ['adynamic bone disease', 'low bone turnover',...      Disease   \n",
       "2                   ['secondary hyperparathyroidism']      Disease   \n",
       "3            ['renal failure', 'renal insufficiency']      Disease   \n",
       "4                   ['secondary hyperparathyroidism']      Disease   \n",
       "5                               ['hyperphosphatemia']      Disease   \n",
       "6            ['renal failure', 'renal insufficiency']      Disease   \n",
       "7   ['adynamic bone disease', 'low bone turnover',...      Disease   \n",
       "8                               ['hyperphosphatemia']      Disease   \n",
       "9                                   ['hypercalcemia']      Disease   \n",
       "10                                  ['hypercalcemia']      Disease   \n",
       "11  ['adynamic bone disease', 'low bone turnover',...      Disease   \n",
       "12                     ['Hypotension', 'hypotension']      Disease   \n",
       "13                                    ['bradycardia']      Disease   \n",
       "14                                       ['asystole']      Disease   \n",
       "15                                ['cardiac disease']      Disease   \n",
       "16                                      ['hypoxemia']      Disease   \n",
       "17                                       ['ischemic']      Disease   \n",
       "18                                   ['sudden death']      Disease   \n",
       "19                       ['pulmonary-renal syndrome']      Disease   \n",
       "\n",
       "   entity2_ann_id  entity2_offset entity2_length relation_type is_novel  \\\n",
       "0              28          1988.0              8   No_Relation  Unknown   \n",
       "1               2            75.0             17   No_Relation  Unknown   \n",
       "2               1            28.0             29   No_Relation  Unknown   \n",
       "3               3           106.0             13   No_Relation  Unknown   \n",
       "4               1            28.0             29   No_Relation  Unknown   \n",
       "5              24          1721.0             17   No_Relation  Unknown   \n",
       "6               3           106.0             13   No_Relation  Unknown   \n",
       "7               2            75.0             17   No_Relation  Unknown   \n",
       "8              24          1721.0             17           CID  Unknown   \n",
       "9               6           273.0             13           CID  Unknown   \n",
       "10              6           273.0             13           CID  Unknown   \n",
       "11              2            75.0             17           CID  Unknown   \n",
       "12              0             0.0             11           CID  Unknown   \n",
       "13              1            13.0             11           CID  Unknown   \n",
       "14              2            30.0              8           CID  Unknown   \n",
       "15              9           367.0             15   No_Relation  Unknown   \n",
       "16             13           513.0              9   No_Relation  Unknown   \n",
       "17              8           358.0              8   No_Relation  Unknown   \n",
       "18             15           803.0             12   No_Relation  Unknown   \n",
       "19             10           432.0             24   No_Relation  Unknown   \n",
       "\n",
       "    row_index  \n",
       "0           0  \n",
       "1           1  \n",
       "2           2  \n",
       "3           3  \n",
       "4           4  \n",
       "5           5  \n",
       "6           6  \n",
       "7           7  \n",
       "8           8  \n",
       "9           9  \n",
       "10         10  \n",
       "11         11  \n",
       "12          0  \n",
       "13          1  \n",
       "14          2  \n",
       "15          3  \n",
       "16          4  \n",
       "17          5  \n",
       "18          6  \n",
       "19          7  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def optimize_index_positions(df, target_pct_cid=0.33, target_pct_no_rel=0.66, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Strategically rearrange rows within each group to minimize deviation from\n",
    "    target label distribution at each index position across all groups.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input dataframe with 'doc_id' and 'relation_type' columns\n",
    "    target_pct_cid : float\n",
    "        Target percentage for CID labels (default 0.33)\n",
    "    target_pct_no_rel : float\n",
    "        Target percentage for No_Relation labels (default 0.66)\n",
    "    max_iterations : int\n",
    "        Maximum optimization iterations (default 100)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with optimized row positions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Get all unique groups\n",
    "    groups = result_df.groupby('doc_id')\n",
    "    \n",
    "    # Store rows by group with their labels\n",
    "    group_data = {}\n",
    "    for doc_id, group in groups:\n",
    "        group_data[doc_id] = group.copy()\n",
    "    \n",
    "    # Find maximum group size to know how many index positions we have\n",
    "    max_group_size = max(len(g) for g in group_data.values())\n",
    "    \n",
    "    # Initialize: assign initial positions (can be random or sequential)\n",
    "    for doc_id in group_data:\n",
    "        group_data[doc_id] = group_data[doc_id].reset_index(drop=True)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for iteration in range(max_iterations):\n",
    "        improved = False\n",
    "        \n",
    "        # For each index position\n",
    "        for idx_pos in range(max_group_size):\n",
    "            # Collect all groups that have this index position\n",
    "            groups_with_idx = {doc_id: g for doc_id, g in group_data.items() \n",
    "                              if len(g) > idx_pos}\n",
    "            \n",
    "            if len(groups_with_idx) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Calculate current distribution at this index\n",
    "            current_labels = [g.iloc[idx_pos]['relation_type'] \n",
    "                            for g in groups_with_idx.values()]\n",
    "            current_cid_count = sum(1 for l in current_labels if l == 'CID')\n",
    "            current_total = len(current_labels)\n",
    "            current_pct = current_cid_count / current_total if current_total > 0 else 0\n",
    "            \n",
    "            # Calculate current deviation from target\n",
    "            current_deviation = abs(current_pct - target_pct_cid)\n",
    "            \n",
    "            # Try swapping within groups to improve this position\n",
    "            group_ids = list(groups_with_idx.keys())\n",
    "            \n",
    "            for i, doc_id in enumerate(group_ids):\n",
    "                group = group_data[doc_id]\n",
    "                current_label_at_idx = group.iloc[idx_pos]['relation_type']\n",
    "                \n",
    "                # Try swapping with other positions in the same group\n",
    "                for swap_pos in range(len(group)):\n",
    "                    if swap_pos == idx_pos:\n",
    "                        continue\n",
    "                    \n",
    "                    swap_label = group.iloc[swap_pos]['relation_type']\n",
    "                    \n",
    "                    # Skip if labels are the same (no point swapping)\n",
    "                    if current_label_at_idx == swap_label:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate what the new distribution would be after swap\n",
    "                    new_labels = current_labels.copy()\n",
    "                    new_labels[i] = swap_label\n",
    "                    new_cid_count = sum(1 for l in new_labels if l == 'CID')\n",
    "                    new_pct = new_cid_count / current_total\n",
    "                    new_deviation = abs(new_pct - target_pct_cid)\n",
    "                    \n",
    "                    # If this swap improves the distribution, do it\n",
    "                    if new_deviation < current_deviation:\n",
    "                        # Perform the swap\n",
    "                        group_data[doc_id].iloc[idx_pos], group_data[doc_id].iloc[swap_pos] = \\\n",
    "                            group_data[doc_id].iloc[swap_pos].copy(), group_data[doc_id].iloc[idx_pos].copy()\n",
    "                        \n",
    "                        current_deviation = new_deviation\n",
    "                        current_labels = new_labels\n",
    "                        improved = True\n",
    "        \n",
    "        # If no improvement in this iteration, we've reached a local optimum\n",
    "        if not improved:\n",
    "            break\n",
    "    \n",
    "    # Combine all groups back together\n",
    "    result_frames = []\n",
    "    for doc_id, group in group_data.items():\n",
    "        group = group.copy()\n",
    "        group['row_index'] = range(len(group))\n",
    "        result_frames.append(group)\n",
    "    \n",
    "    result_df = pd.concat(result_frames, ignore_index=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def greedy_index_balance(df, target_pct_cid=0.33):\n",
    "    \"\"\"\n",
    "    Alternative greedy approach: build the arrangement from scratch,\n",
    "    placing each row in the position that best balances the distribution.\n",
    "    \n",
    "    This is faster but may not find the absolute best solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group by doc_id\n",
    "    groups = {}\n",
    "    for doc_id, group in df.groupby('doc_id'):\n",
    "        groups[doc_id] = group.copy().reset_index(drop=True)\n",
    "    \n",
    "    # Find max group size\n",
    "    max_size = max(len(g) for g in groups.values())\n",
    "    \n",
    "    # For each group, create a list to hold the final order\n",
    "    final_orders = {doc_id: [None] * len(group) for doc_id, group in groups.items()}\n",
    "    \n",
    "    # Track which rows have been placed\n",
    "    placed = {doc_id: [False] * len(group) for doc_id, group in groups.items()}\n",
    "    \n",
    "    # For each index position, greedily assign rows\n",
    "    for idx_pos in range(max_size):\n",
    "        # Track current distribution at this position\n",
    "        position_labels = []\n",
    "        \n",
    "        # Get groups that have this position available\n",
    "        for doc_id, group in groups.items():\n",
    "            if len(group) <= idx_pos:\n",
    "                continue\n",
    "            \n",
    "            # Find best unplaced row to put at this position\n",
    "            best_row_idx = None\n",
    "            best_deviation = float('inf')\n",
    "            \n",
    "            for row_idx in range(len(group)):\n",
    "                if placed[doc_id][row_idx]:\n",
    "                    continue\n",
    "                \n",
    "                # Try this row at this position\n",
    "                test_label = group.iloc[row_idx]['relation_type']\n",
    "                test_labels = position_labels + [test_label]\n",
    "                \n",
    "                # Calculate deviation\n",
    "                cid_count = sum(1 for l in test_labels if l == 'CID')\n",
    "                total = len(test_labels)\n",
    "                pct = cid_count / total if total > 0 else 0\n",
    "                deviation = abs(pct - target_pct_cid)\n",
    "                \n",
    "                if deviation < best_deviation:\n",
    "                    best_deviation = deviation\n",
    "                    best_row_idx = row_idx\n",
    "            \n",
    "            # Place the best row at this position\n",
    "            if best_row_idx is not None:\n",
    "                final_orders[doc_id][idx_pos] = group.iloc[best_row_idx]\n",
    "                placed[doc_id][best_row_idx] = True\n",
    "                position_labels.append(group.iloc[best_row_idx]['relation_type'])\n",
    "    \n",
    "    # Reconstruct dataframe\n",
    "    result_frames = []\n",
    "    for doc_id, ordered_rows in final_orders.items():\n",
    "        # Filter out None values (for groups smaller than max_size)\n",
    "        ordered_rows = [r for r in ordered_rows if r is not None]\n",
    "        if ordered_rows:\n",
    "            group_df = pd.DataFrame(ordered_rows)\n",
    "            group_df['row_index'] = range(len(group_df))\n",
    "            result_frames.append(group_df)\n",
    "    \n",
    "    return pd.concat(result_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "# Example usage to replace your random shuffle:\n",
    "# Instead of:\n",
    "# shuffled_df = balanced_df_cut.groupby('doc_id').apply(lambda x: x.sample(frac=1, random_state=np.random.randint(1, 100))).reset_index(drop=True)\n",
    "\n",
    "# Use:\n",
    "#optimized_df = optimize_index_positions(balanced_df_cut, target_pct_cid=0.33, target_pct_no_rel=0.66)\n",
    "\n",
    "# Or for faster execution:\n",
    "optimized_df = greedy_index_balance(balanced_df_cut, target_pct_cid=0.33)\n",
    "show_index_table(optimized_df)\n",
    "print_summary_df(optimized_df)\n",
    "optimized_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec31175c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced data:\n",
      "relation_type        CID  No_Relation\n",
      "row_index                            \n",
      "0              37.954545    62.045455\n",
      "1              34.482759    65.517241\n",
      "2              29.691877    70.308123\n",
      "3              33.762058    66.237942\n",
      "4              31.617647    68.382353\n",
      "5              33.018868    66.981132\n",
      "6              34.911243    65.088757\n",
      "7              42.148760    57.851240\n",
      "8              38.750000    61.250000\n",
      "9              35.820896    64.179104\n",
      "10             40.740741    59.259259\n",
      "11             30.952381    69.047619\n",
      "12             26.470588    73.529412\n",
      "13             22.580645    77.419355\n",
      "14             25.925926    74.074074\n",
      "15             16.666667    83.333333\n",
      "16             25.000000    75.000000\n",
      "17             29.411765    70.588235\n",
      "18             12.500000    87.500000\n",
      "19             20.000000    80.000000\n",
      "20             20.000000    80.000000\n",
      "21             22.222222    77.777778\n",
      "22             25.000000    75.000000\n",
      "23             16.666667    83.333333\n",
      "24             20.000000    80.000000\n",
      "25              0.000000   100.000000\n",
      "26              0.000000   100.000000\n",
      "27             33.333333    66.666667\n",
      "28             33.333333    66.666667\n",
      "29             33.333333    66.666667\n",
      "30             33.333333    66.666667\n",
      "31             33.333333    66.666667\n",
      "32             33.333333    66.666667\n",
      "33             33.333333    66.666667\n",
      "34              0.000000   100.000000\n",
      "35              0.000000   100.000000\n",
      "36              0.000000   100.000000\n",
      "37              0.000000   100.000000\n",
      "\n",
      "Final filtered data:\n",
      "    Count  Percentage        CID  No_Relation\n",
      "1      34    7.727273  88.235294    11.764706\n",
      "2      49   11.136364  48.979592    51.020408\n",
      "3      46   10.454545  50.724638    49.275362\n",
      "4      39    8.863636  42.307692    57.692308\n",
      "5      60   13.636364  37.333333    62.666667\n",
      "6      43    9.772727  34.496124    65.503876\n",
      "7      89   20.227273  29.373997    70.626003\n",
      "8      13    2.954545  33.653846    66.346154\n",
      "9      25    5.681818  32.888889    67.111111\n",
      "10     25    5.681818  21.200000    78.800000\n",
      "11     14    3.181818  22.727273    77.272727\n",
      "18      3    0.681818  33.333333    66.666667\n",
      "relation_type        CID  No_Relation\n",
      "row_index                            \n",
      "0              37.954545    62.045455\n",
      "1              34.482759    65.517241\n",
      "2              29.691877    70.308123\n",
      "3              33.762058    66.237942\n",
      "4              31.617647    68.382353\n",
      "5              33.018868    66.981132\n",
      "6              34.911243    65.088757\n",
      "7              38.750000    61.250000\n",
      "8              35.820896    64.179104\n",
      "9              30.952381    69.047619\n",
      "10             29.411765    70.588235\n",
      "11             33.333333    66.666667\n",
      "12             33.333333    66.666667\n",
      "13             33.333333    66.666667\n",
      "14             33.333333    66.666667\n",
      "15             33.333333    66.666667\n",
      "16             33.333333    66.666667\n",
      "17             33.333333    66.666667\n"
     ]
    }
   ],
   "source": [
    "def balance_labels_by_index(df):\n",
    "    \"\"\"\n",
    "    Balance labels across groups by index position.\n",
    "    \n",
    "    For each index position across all groups:\n",
    "    - Drop labels until achieving 33% CID and 66% No_Relation ratio\n",
    "    - Drop from groups with most entries first\n",
    "    - If ratio can't be achieved, drop the entire index position\n",
    "    - Re-index groups after balancing\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns 'doc_id', 'relation_type'\n",
    "        target_cid_ratio: Target ratio for CID labels (default 0.33)\n",
    "        target_no_relation_ratio: Target ratio for No_Relation labels (default 0.66)\n",
    "    \n",
    "    Returns:\n",
    "        Balanced DataFrame with re-indexed groups\n",
    "    \"\"\"\n",
    "    # Add index within each doc_id group\n",
    "    df = df.copy()\n",
    "    df['group_index'] = df.groupby('doc_id').cumcount()\n",
    "    \n",
    "    # Get group sizes for sorting (drop from largest groups first)\n",
    "    group_sizes = df.groupby('doc_id').size().to_dict()\n",
    "    df['group_size'] = df['doc_id'].map(group_sizes)\n",
    "    \n",
    "    rows_to_keep = []\n",
    "    \n",
    "    # Process each index position\n",
    "    max_index = df['group_index'].max()\n",
    "    \n",
    "    for idx in range(max_index + 1):\n",
    "        idx_data = df[df['group_index'] == idx].copy()\n",
    "        \n",
    "        if len(idx_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Count labels at this index\n",
    "        label_counts = idx_data['relation_type'].value_counts()\n",
    "        \n",
    "        # Check if we have both CID and No_Relation\n",
    "        if 'CID' not in label_counts or 'No_Relation' not in label_counts:\n",
    "            # Skip this index if we don't have both required labels\n",
    "            continue\n",
    "        \n",
    "        cid_count = label_counts.get('CID', 0)\n",
    "        no_relation_count = label_counts.get('No_Relation', 0)\n",
    "        other_count = len(idx_data) - cid_count - no_relation_count\n",
    "        \n",
    "        # Calculate target counts based on CID count\n",
    "        # If we have X CID labels, we need 2X No_Relation labels (to get 33%/66% ratio)\n",
    "\n",
    "        target_no_relation = int(cid_count * 2)\n",
    "   \n",
    "        target_cid_relation = int(no_relation_count / 2)\n",
    "        \n",
    "        \n",
    "        # Sort by group_size (descending) so we drop from largest groups first\n",
    "        idx_data = idx_data.sort_values('group_size', ascending=False)\n",
    "        \n",
    "        # filter CID\n",
    "        cid_rows = idx_data[idx_data['relation_type'] == 'CID'].head(target_cid_relation) # this does nothing why ?\n",
    "        \n",
    "        # Keep only target_no_relation No_Relation labels\n",
    "        no_relation_rows = idx_data[idx_data['relation_type'] == 'No_Relation'].head(target_no_relation)\n",
    "        \n",
    "        # Drop all other labels\n",
    "        balanced_idx_data = pd.concat([cid_rows, no_relation_rows])\n",
    "        \n",
    "        rows_to_keep.append(balanced_idx_data)\n",
    "    \n",
    "    # Combine all kept rows\n",
    "    if not rows_to_keep:\n",
    "        return pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    balanced_df = pd.concat(rows_to_keep, ignore_index=True)\n",
    "    \n",
    "    # Remove helper columns\n",
    "    balanced_df = balanced_df.drop(columns=['group_index', 'group_size'])\n",
    "    \n",
    "    # Re-index within each doc_id group\n",
    "    balanced_df['new_index'] = balanced_df.groupby('doc_id').cumcount()\n",
    "    \n",
    "    # Optional: sort by doc_id and new_index for clarity\n",
    "    balanced_df = balanced_df.sort_values(['doc_id', 'new_index']).reset_index(drop=True)\n",
    "    \n",
    "    # Drop the new_index column if you don't need it in the output\n",
    "    balanced_df = balanced_df.drop(columns=['new_index'])\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "balanced_df = balance_labels_by_index(df)\n",
    "\n",
    "# Check the results\n",
    "#print(\"Original data:\")\n",
    "#print_summary_df(df)\n",
    "print(\"\\nBalanced data:\")\n",
    "#print_summary_df(balanced_df)\n",
    "show_index_table(balanced_df)\n",
    "\n",
    "def filter_by_no_relation_percentage(df, min_pct=61, max_pct=71):\n",
    "    \"\"\"\n",
    "    Drop entire index positions across all doc_ids if the No_Relation \n",
    "    percentage is less than min_pct or more than max_pct.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns 'doc_id', 'relation_type'\n",
    "        min_pct: Minimum percentage of No_Relation labels (default 61)\n",
    "        max_pct: Maximum percentage of No_Relation labels (default 71)\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    # Add index within each doc_id group\n",
    "    df = df.copy()\n",
    "    df['group_index'] = df.groupby('doc_id').cumcount()\n",
    "    \n",
    "    # Calculate No_Relation percentage for each index position\n",
    "    index_stats = df.groupby('group_index')['relation_type'].apply(\n",
    "        lambda x: (x == 'No_Relation').sum() / len(x) * 100\n",
    "    ).reset_index()\n",
    "    index_stats.columns = ['group_index', 'no_relation_pct']\n",
    "    \n",
    "    # Find valid index positions\n",
    "    valid_indices = index_stats[\n",
    "        (index_stats['no_relation_pct'] >= min_pct) & \n",
    "        (index_stats['no_relation_pct'] <= max_pct)\n",
    "    ]['group_index'].values\n",
    "    \n",
    "    # Filter dataframe to keep only valid index positions\n",
    "    filtered_df = df[df['group_index'].isin(valid_indices)].copy()\n",
    "    \n",
    "    # Remove helper column\n",
    "    filtered_df = filtered_df.drop(columns=['group_index'])\n",
    "    \n",
    "    # Re-index within each doc_id group\n",
    "    filtered_df['new_index'] = filtered_df.groupby('doc_id').cumcount()\n",
    "    filtered_df = filtered_df.sort_values(['doc_id', 'new_index']).reset_index(drop=True)\n",
    "    filtered_df = filtered_df.drop(columns=['new_index'])\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "# Usage:\n",
    "balanced_df = balance_labels_by_index(df)\n",
    "final_df = filter_by_no_relation_percentage(balanced_df, min_pct=61, max_pct=71)\n",
    "\n",
    "print(\"\\nFinal filtered data:\")\n",
    "print_summary_df(final_df)\n",
    "show_index_table(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cde5f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Prompt     Relation     DocID  \\\n",
      "0  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "1  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "2  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "3  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "4  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "\n",
      "   run  prompt_id  \n",
      "0    1          1  \n",
      "1    1          2  \n",
      "2    1          3  \n",
      "3    1          4  \n",
      "4    1          5  \n",
      "                                              Prompt     Relation     DocID  \\\n",
      "0  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "1  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "2  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "3  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "4  You are a biomedical relation extraction exper...  No_Relation  10027919   \n",
      "\n",
      "   run  prompt_id  \n",
      "0    1          1  \n",
      "1    1          2  \n",
      "2    1          3  \n",
      "3    1          4  \n",
      "4    1          5  \n",
      "                                              Prompt  \\\n",
      "0  You are a biomedical relation extraction exper...   \n",
      "1  You are a biomedical relation extraction exper...   \n",
      "2  You are a biomedical relation extraction exper...   \n",
      "3  You are a biomedical relation extraction exper...   \n",
      "4  You are a biomedical relation extraction exper...   \n",
      "\n",
      "                                     Multi_Relations Multi_DocID  run  \n",
      "0  [No_Relation, No_Relation, No_Relation, No_Rel...    10027919    1  \n",
      "1  [CID, CID, CID, No_Relation, No_Relation, No_R...    10074612    1  \n",
      "2                    [No_Relation, No_Relation, CID]    10091616    1  \n",
      "3                    [No_Relation, No_Relation, CID]    10091617    1  \n",
      "4                    [CID, No_Relation, No_Relation]     1009330    1  \n"
     ]
    }
   ],
   "source": [
    "template_path_multi = \"LLM_benchmarks/prompt_template_multi.txt\"\n",
    "template_path_single = \"LLM_benchmarks/prompt_template_single.txt\"\n",
    "save_as = \"LLM_benchmarks/Prompts/few_shot_single_for_api.json\"\n",
    "merged_df_train = optimized_df\n",
    "zero_shot_single = create_prompt_relation_df_single(merged_df_train,merged_df_test, template_path_single, examples=False,same_examples=True, seed=42, n_prompts=3)\n",
    "print(zero_shot_single.head())\n",
    "import json\n",
    "prompt_json = prompts_df_to_json_from_runs(zero_shot_single)\n",
    "with open(\"LLM_benchmarks/Prompts/zero_shot_single_for_api_2_CID.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2)\n",
    "\n",
    "few_shot_single = create_prompt_relation_df_single(merged_df_train,merged_df_test, template_path_single, examples=True,same_examples=True, seed=42, n_prompts=3)\n",
    "print(few_shot_single.head())\n",
    "prompt_json = prompts_df_to_json_from_runs(few_shot_single)\n",
    "with open(\"LLM_benchmarks/Prompts/few_shot_single_for_api_2_CID.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2)\n",
    "\n",
    "\n",
    "# Run garbage collection to free up memory\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "zero_shot_multi = create_prompt_relation_df_multi_by_docid(merged_df_train,merged_df_test, template_path_multi, examples=False,same_examples=True, seed=42, n_prompts=3, number_relations=True, relations_entitys_separately=False)\n",
    "prompt_json = prompts_df_to_json_from_runs_multi(zero_shot_multi)\n",
    "with open(\"LLM_benchmarks/Prompts/zero_shot_multi_for_api_2_CID.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2)\n",
    "\n",
    "few_shot_multi = create_prompt_relation_df_multi_by_docid(merged_df_train,merged_df_test, template_path_multi, examples=True,same_examples=True, seed=42, n_prompts=3, number_relations=True, relations_entitys_separately=False)\n",
    "prompt_json = prompts_df_to_json_from_runs_multi(few_shot_multi)\n",
    "with open(\"LLM_benchmarks/Prompts/few_shot_multi_for_api_2_CID.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2) \n",
    "print(few_shot_multi.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57e6dd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: zero_shot_multi_for_api_shuffled.json\n",
      "Saved: few_shot_multi_for_api_shuffled.json\n"
     ]
    }
   ],
   "source": [
    "# Generate SHUFFLED versions of multi prompts\n",
    "# The shuffle=True parameter will shuffle entity pairs within each doc_id\n",
    "# Different runs will have different shuffle orders for the same doc_id\n",
    "\n",
    "# Zero-shot multi with shuffled entity order\n",
    "zero_shot_multi_shuffled = create_prompt_relation_df_multi_by_docid(\n",
    "    merged_df_train, merged_df_test, template_path_multi,\n",
    "    examples=False, same_examples=False,\n",
    "    seed=42, n_prompts=3,\n",
    "    number_relations=True, relations_entitys_separately=False,\n",
    "    shuffle=False\n",
    " # Enable shuffling\n",
    ")\n",
    "prompt_json = prompts_df_to_json_from_runs_multi(zero_shot_multi_shuffled)\n",
    "with open(\"LLM_benchmarks/Prompts/zero_shot_multi_for_api_shuffled_2_even.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2)\n",
    "print(\"Saved: zero_shot_multi_for_api_shuffled.json\")\n",
    "\n",
    "# Few-shot multi with shuffled entity order\n",
    "few_shot_multi_shuffled = create_prompt_relation_df_multi_by_docid(\n",
    "    merged_df_train, merged_df_test, template_path_multi,\n",
    "    examples=True, same_examples=True,\n",
    "    seed=42, n_prompts=3,\n",
    "    number_relations=True, relations_entitys_separately=False,\n",
    "    shuffle=False\n",
    ")\n",
    "prompt_json = prompts_df_to_json_from_runs_multi(few_shot_multi_shuffled)\n",
    "with open(\"LLM_benchmarks/Prompts/few_shot_multi_for_api_shuffled_2_even.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2)\n",
    "print(\"Saved: few_shot_multi_for_api_shuffled.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155e71f",
   "metadata": {},
   "source": [
    "## Make 1 big json for the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fca1d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_prompt_jsons(json_paths, prompt_set_names, output_path):\n",
    "    \"\"\"\n",
    "    Merge multiple prompt JSONs into one, each as a different prompt set name.\n",
    "    Also adds Multi_Relations and Multi_DocID fields if present and not already in the base.\n",
    "    \"\"\"\n",
    "    import json\n",
    "\n",
    "    # Load all JSONs\n",
    "    jsons = [json.load(open(p, \"r\", encoding=\"utf-8\")) for p in json_paths]\n",
    "    base = jsons[0]\n",
    "    n_key = list(base.keys())[0]\n",
    "    runs = base[n_key].keys()\n",
    "\n",
    "    # For each run, add all prompt sets and Multi fields if present\n",
    "    for run in runs:\n",
    "        for i, js in enumerate(jsons):\n",
    "            prompt_set_name = prompt_set_names[i]\n",
    "            # Find the first prompt set key in this json (e.g. \"prompts1\")\n",
    "            prompt_set_key = [k for k in js[n_key][run].keys() if k.startswith(\"prompts\")][0]\n",
    "            base[n_key][run][prompt_set_name] = js[n_key][run][prompt_set_key]\n",
    "            # Add Multi_Relations if present and not already in base\n",
    "            if \"multi_relations\" in js[n_key][run] and \"multi_relations\" not in base[n_key][run]:\n",
    "                base[n_key][run][\"multi_relations\"] = js[n_key][run][\"multi_relations\"]\n",
    "            # Add Multi_DocID if present and not already in base\n",
    "            if \"multi_docid\" in js[n_key][run] and \"multi_docid\" not in base[n_key][run]:\n",
    "                base[n_key][run][\"multi_docid\"] = js[n_key][run][\"multi_docid\"]\n",
    "\n",
    "    # Save merged JSON\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(base, f, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "consolidate_prompt_jsons(\n",
    "    [\n",
    "        \"LLM_benchmarks/Prompts/zero_shot_single_for_api_2_CID.json\",\n",
    "        \"LLM_benchmarks/Prompts/few_shot_single_for_api_2_CID.json\",\n",
    "        \"LLM_benchmarks/Prompts/zero_shot_multi_for_api_shuffled_2_even.json\",\n",
    "        \"LLM_benchmarks/Prompts/few_shot_multi_for_api_shuffled_2_even.json\"\n",
    "    ],\n",
    "    [\"single_zero\", \"single_examples\", \"multi_zero\", \"multi_examples\"],\n",
    "    \"LLM_benchmarks/Prompts/Test_callibration_CID_Shuffeled_3_even.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e88caa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 16)\n",
      "['10579464' '11250767' '12523465' '12596116' '12789195' '15036754'\n",
      " '15804801' '17042910' '18208574' '2870085' '3856631' '4812392' '6103707'\n",
      " '6209318' '6538499' '6747681' '7292072' '7420681' '8682684' '983936']\n",
      "                                              Prompt     Relation     DocID  \\\n",
      "0  You are a biomedical relation extraction exper...  No_Relation  10579464   \n",
      "1  You are a biomedical relation extraction exper...  No_Relation  10579464   \n",
      "2  You are a biomedical relation extraction exper...  No_Relation  10579464   \n",
      "3  You are a biomedical relation extraction exper...          CID  10579464   \n",
      "4  You are a biomedical relation extraction exper...  No_Relation  10579464   \n",
      "\n",
      "   run  prompt_id  \n",
      "0    1          1  \n",
      "1    1          2  \n",
      "2    1          3  \n",
      "3    1          4  \n",
      "4    1          5  \n",
      "                                              Prompt     Relation     DocID  \\\n",
      "0  You are a biomedical relation extraction exper...  No_Relation  10579464   \n",
      "1  You are a biomedical relation extraction exper...  No_Relation  10579464   \n",
      "2  You are a biomedical relation extraction exper...  No_Relation  10579464   \n",
      "3  You are a biomedical relation extraction exper...          CID  10579464   \n",
      "4  You are a biomedical relation extraction exper...  No_Relation  10579464   \n",
      "\n",
      "   run  prompt_id  \n",
      "0    1          1  \n",
      "1    1          2  \n",
      "2    1          3  \n",
      "3    1          4  \n",
      "4    1          5  \n",
      "                                              Prompt  \\\n",
      "0  You are a biomedical relation extraction exper...   \n",
      "1  You are a biomedical relation extraction exper...   \n",
      "2  You are a biomedical relation extraction exper...   \n",
      "3  You are a biomedical relation extraction exper...   \n",
      "4  You are a biomedical relation extraction exper...   \n",
      "\n",
      "                                     Multi_Relations Multi_DocID  run  \n",
      "0  [No_Relation, No_Relation, No_Relation, CID, N...    10579464    1  \n",
      "1  [No_Relation, CID, No_Relation, CID, CID, No_R...    11250767    1  \n",
      "2                    [CID, No_Relation, No_Relation]    12523465    1  \n",
      "3       [No_Relation, CID, No_Relation, No_Relation]    12596116    1  \n",
      "4  [No_Relation, CID, No_Relation, No_Relation, N...    12789195    1  \n",
      "                                              Prompt  \\\n",
      "0  You are a biomedical relation extraction exper...   \n",
      "1  You are a biomedical relation extraction exper...   \n",
      "2  You are a biomedical relation extraction exper...   \n",
      "3  You are a biomedical relation extraction exper...   \n",
      "4  You are a biomedical relation extraction exper...   \n",
      "\n",
      "                                     Multi_Relations Multi_DocID  run  \n",
      "0  [No_Relation, No_Relation, No_Relation, CID, N...    10579464    1  \n",
      "1  [No_Relation, CID, No_Relation, CID, CID, No_R...    11250767    1  \n",
      "2                    [CID, No_Relation, No_Relation]    12523465    1  \n",
      "3       [No_Relation, CID, No_Relation, No_Relation]    12596116    1  \n",
      "4  [No_Relation, CID, No_Relation, No_Relation, N...    12789195    1  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set your seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Get unique doc_ids\n",
    "unique_docids = merged_df_train['doc_id'].unique()\n",
    "\n",
    "# Pick 20 random doc_ids\n",
    "sampled_docids = np.random.choice(unique_docids, size=20, replace=False)\n",
    "\n",
    "# Subset the DataFrame to only rows with those doc_ids\n",
    "subset_df = merged_df_train[merged_df_train['doc_id'].isin(sampled_docids)].copy()\n",
    "\n",
    "print(subset_df.shape)\n",
    "print(subset_df['doc_id'].unique())\n",
    "\n",
    "template_path_multi = \"LLM_benchmarks/prompt_template_multi_CID.txt\"\n",
    "template_path_single = \"LLM_benchmarks/prompt_template_single_CID.txt\"\n",
    "#template_path_single_wo_reason = \"LLM_benchmarks/prompt_template_single_wo_reason.txt\"\n",
    "\n",
    "test_single = create_prompt_relation_df_single(subset_df,merged_df_test, template_path_single, examples=False,same_examples=False, seed=42, n_prompts=25, prompt_id=False)\n",
    "print(test_single.head())\n",
    "import json\n",
    "prompt_json = prompts_df_to_json_from_runs(test_single)\n",
    "with open(\"LLM_benchmarks/Prompts/test_single_for_api_CID_2.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2)\n",
    "\n",
    "test_single_w_examples = create_prompt_relation_df_single(subset_df,merged_df_test, template_path_single, examples=True,same_examples=True, seed=42, n_prompts=25, prompt_id=False)\n",
    "print(test_single_w_examples.head())\n",
    "import json\n",
    "prompt_json = prompts_df_to_json_from_runs(test_single_w_examples)\n",
    "with open(\"LLM_benchmarks/Prompts/test_single_for_api_examples_CID_2.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2)\n",
    "\n",
    "test_multi = create_prompt_relation_df_multi_by_docid(subset_df,merged_df_test, template_path_multi, examples=False,same_examples=False, seed=42, n_prompts=25, number_relations=True, relations_entitys_separately=False)\n",
    "print(test_multi.head())\n",
    "import json\n",
    "prompt_json = prompts_df_to_json_from_runs_multi(test_multi)\n",
    "with open(\"LLM_benchmarks/Prompts/test_multi_for_api_CID_2.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2) \n",
    "\n",
    "test_multi_w_examples = create_prompt_relation_df_multi_by_docid(subset_df,merged_df_test, template_path_multi, examples=True,same_examples=True, seed=42, n_prompts=25, number_relations=True, relations_entitys_separately=False)\n",
    "print(test_multi_w_examples.head())\n",
    "import json\n",
    "prompt_json = prompts_df_to_json_from_runs_multi(test_multi_w_examples)\n",
    "with open(\"LLM_benchmarks/Prompts/test_multi_for_api_examples_CID_2.json\", \"w\") as f:\n",
    "    json.dump(prompt_json, f, indent=2)\n",
    "\n",
    "consolidate_prompt_jsons(\n",
    "    [\n",
    "        \"LLM_benchmarks/Prompts/test_single_for_api_CID_2.json\",\n",
    "        \"LLM_benchmarks/Prompts/test_single_for_api_examples_CID_2.json\",\n",
    "        \"LLM_benchmarks/Prompts/test_multi_for_api_CID_2.json\",\n",
    "        \"LLM_benchmarks/Prompts/test_multi_for_api_examples_CID_2.json\"\n",
    "    ],\n",
    "    [\"single_zero\", \"single_examples\", \"multi_zero\", \"multi_examples\"],\n",
    "    \"LLM_benchmarks/Prompts/Test_callibration_CID_long.json\"\n",
    ")\n",
    "#calibration_test = json.load(open(\"LLM_benchmarks/Prompts/Test_callibration_CID.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f176084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a biomedical relation extraction expert.\n",
      "Prompt_ID:None\n",
      "Task: Identify if the realtion between two specified entities is chemically inducing a disease or symptoms of a disease.\n",
      "\n",
      "Valid relation types (respond with EXACTLY ONE of these words):\n",
      "- CID: stands for Chemically induced Disease, give this label if the two enteties are a chemical and a disease where the chemical induces the disease or its symptoms. \n",
      "- No_Relation: The two enteties are according to the text not involved in chemically inducing a Disease.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Only annotate EXPLICIT relations stated in the text\n",
      "2. If entities co-occur WITHOUT a clear relationship  respond with \"No_Relation\"\n",
      "3. Do NOT infer or assume relationships not explicitly stated\n",
      "4. When uncertain  respond with \"No_Relation\"\n",
      "Example 1\n",
      "## Input:\n",
      "Text: Intraoperative bradycardia and hypotension associated with timolol and pilocarpine eye drops. A 69-yr-old man, who was concurrently being treated with pilocarpine nitrate and timolol maleate eye drops, developed a bradycardia and became hypotensive during halothane anaesthesia. Both timolol and pilocarpine were subsequently identified in a 24-h collection of urine. Timolol (but not pilocarpine) was detected in a sample of plasma removed during surgery; the plasma concentration of timolol (2.6 ng ml-1) was consistent with partial beta-adrenoceptor blockade. It is postulated that this action may have been enhanced during halothane anaesthesia with resultant bradycardia and hypotension. Pilocarpine may have had a contributory effect.\n",
      "Entities:\n",
      "1. Entity1: ['Timolol', 'timolol', 'timolol maleate'] (Chemical), Entity2: ['bradycardia'] (Disease)\n",
      "2. Entity1: ['Timolol', 'timolol', 'timolol maleate'] (Chemical), Entity2: ['hypotension', 'hypotensive'] (Disease)\n",
      "3. Entity1: ['Pilocarpine', 'pilocarpine', 'pilocarpine nitrate'] (Chemical), Entity2: ['bradycardia'] (Disease)\n",
      "4. Entity1: ['Pilocarpine', 'pilocarpine', 'pilocarpine nitrate'] (Chemical), Entity2: ['hypotension', 'hypotensive'] (Disease)\n",
      "5. Entity1: ['halothane'] (Chemical), Entity2: ['bradycardia'] (Disease)\n",
      "6. Entity1: ['halothane'] (Chemical), Entity2: ['hypotension', 'hypotensive'] (Disease)\n",
      "## Output:\n",
      "1. Entity1: ['Timolol', 'timolol', 'timolol maleate'], Entity2: ['bradycardia'], Relation: CID\n",
      "2. Entity1: ['Timolol', 'timolol', 'timolol maleate'], Entity2: ['hypotension', 'hypotensive'], Relation: CID\n",
      "3. Entity1: ['Pilocarpine', 'pilocarpine', 'pilocarpine nitrate'], Entity2: ['bradycardia'], Relation: CID\n",
      "4. Entity1: ['Pilocarpine', 'pilocarpine', 'pilocarpine nitrate'], Entity2: ['hypotension', 'hypotensive'], Relation: CID\n",
      "5. Entity1: ['halothane'], Entity2: ['bradycardia'], Relation: No_Relation\n",
      "6. Entity1: ['halothane'], Entity2: ['hypotension', 'hypotensive'], Relation: No_Relation\n",
      "\n",
      "Example 2\n",
      "## Input:\n",
      "Text: A pilot study to assess the safety of dobutamine stress echocardiography in the emergency department evaluation of cocaine-associated chest pain. STUDY OBJECTIVE: Chest pain in the setting of cocaine use poses a diagnostic dilemma. Dobutamine stress echocardiography (DSE) is a widely available and sensitive test for evaluating cardiac ischemia. Because of the theoretical concern regarding administration of dobutamine in the setting of cocaine use, we conducted a pilot study to assess the safety of DSE in emergency department patients with cocaine-associated chest pain. METHODS: A prospective case series was conducted in the intensive diagnostic and treatment unit in the ED of an urban tertiary-care teaching hospital. Patients were eligible for DSE if they had used cocaine within 24 hours preceding the onset of chest pain and had a normal ECG and tropinin I level. Patients exhibiting signs of continuing cocaine toxicity were excluded from the study. All patients were admitted to the hospital for serial testing after the DSE testing in the intensive diagnostic and treatment unit. RESULTS: Twenty-four patients were enrolled. Two patients had inadequate resting images, one DSE was terminated because of inferior hypokinesis, another DSE was terminated because of a rate-related atrial conduction deficit, and 1 patient did not reach the target heart rate. Thus, 19 patients completed a DSE and reached their target heart rates. None of the patients experienced signs of exaggerated adrenergic response, which was defined as a systolic blood pressure of greater than 200 mm Hg or the occurrence of tachydysrhythmias (excluding sinus tachycardia). Further suggesting lack of exaggerated adrenergic response, 13 (65%) of 20 patients required supplemental atropine to reach their target heart rates. CONCLUSION: No exaggerated adrenergic response was detected when dobutamine was administered to patients with cocaine-related chest pain.\n",
      "Entities:\n",
      "1. Entity1: ['Dobutamine', 'dobutamine'] (Chemical), Entity2: ['Chest pain', 'chest pain'] (Disease)\n",
      "2. Entity1: ['Dobutamine', 'dobutamine'] (Chemical), Entity2: ['ischemia'] (Disease)\n",
      "3. Entity1: ['Dobutamine', 'dobutamine'] (Chemical), Entity2: ['toxicity'] (Disease)\n",
      "4. Entity1: ['Dobutamine', 'dobutamine'] (Chemical), Entity2: ['hypokinesis'] (Disease)\n",
      "5. Entity1: ['Dobutamine', 'dobutamine'] (Chemical), Entity2: ['sinus tachycardia'] (Disease)\n",
      "6. Entity1: ['cocaine'] (Chemical), Entity2: ['Chest pain', 'chest pain'] (Disease)\n",
      "7. Entity1: ['cocaine'] (Chemical), Entity2: ['ischemia'] (Disease)\n",
      "8. Entity1: ['cocaine'] (Chemical), Entity2: ['toxicity'] (Disease)\n",
      "9. Entity1: ['cocaine'] (Chemical), Entity2: ['hypokinesis'] (Disease)\n",
      "10. Entity1: ['cocaine'] (Chemical), Entity2: ['sinus tachycardia'] (Disease)\n",
      "11. Entity1: ['atropine'] (Chemical), Entity2: ['Chest pain', 'chest pain'] (Disease)\n",
      "12. Entity1: ['atropine'] (Chemical), Entity2: ['ischemia'] (Disease)\n",
      "13. Entity1: ['atropine'] (Chemical), Entity2: ['toxicity'] (Disease)\n",
      "14. Entity1: ['atropine'] (Chemical), Entity2: ['hypokinesis'] (Disease)\n",
      "15. Entity1: ['atropine'] (Chemical), Entity2: ['sinus tachycardia'] (Disease)\n",
      "## Output:\n",
      "1. Entity1: ['Dobutamine', 'dobutamine'], Entity2: ['Chest pain', 'chest pain'], Relation: No_Relation\n",
      "2. Entity1: ['Dobutamine', 'dobutamine'], Entity2: ['ischemia'], Relation: No_Relation\n",
      "3. Entity1: ['Dobutamine', 'dobutamine'], Entity2: ['toxicity'], Relation: No_Relation\n",
      "4. Entity1: ['Dobutamine', 'dobutamine'], Entity2: ['hypokinesis'], Relation: No_Relation\n",
      "5. Entity1: ['Dobutamine', 'dobutamine'], Entity2: ['sinus tachycardia'], Relation: No_Relation\n",
      "6. Entity1: ['cocaine'], Entity2: ['Chest pain', 'chest pain'], Relation: CID\n",
      "7. Entity1: ['cocaine'], Entity2: ['ischemia'], Relation: No_Relation\n",
      "8. Entity1: ['cocaine'], Entity2: ['toxicity'], Relation: No_Relation\n",
      "9. Entity1: ['cocaine'], Entity2: ['hypokinesis'], Relation: No_Relation\n",
      "10. Entity1: ['cocaine'], Entity2: ['sinus tachycardia'], Relation: No_Relation\n",
      "11. Entity1: ['atropine'], Entity2: ['Chest pain', 'chest pain'], Relation: No_Relation\n",
      "12. Entity1: ['atropine'], Entity2: ['ischemia'], Relation: No_Relation\n",
      "13. Entity1: ['atropine'], Entity2: ['toxicity'], Relation: No_Relation\n",
      "14. Entity1: ['atropine'], Entity2: ['hypokinesis'], Relation: No_Relation\n",
      "15. Entity1: ['atropine'], Entity2: ['sinus tachycardia'], Relation: No_Relation\n",
      "\n",
      "---\n",
      "\n",
      "Analyze this case:\n",
      "Text: A selective dopamine D4 receptor antagonist, NRA0160: a preclinical neuropharmacological profile. NRA0160, 5 - [2- ( 4- ( 3 - fluorobenzylidene) piperidin-1-yl) ethyl] - 4 -(4-fluorophenyl) thiazole-2-carboxamide, has a high affinity for human cloned dopamine D4.2, D4.4 and D4.7 receptors, with Ki values of 0.5, 0.9 and 2.7 nM, respectively. NRA0160 is over 20,000fold more potent at the dopamine D4.2 receptor compared with the human cloned dopamine D2L receptor. NRA0160 has negligible affinity for the human cloned dopamine D3 receptor (Ki=39 nM), rat serotonin (5-HT)2A receptors (Ki=180 nM) and rat alpha1 adrenoceptor (Ki=237 nM). NRA0160 and clozapine antagonized locomotor hyperactivity induced by methamphetamine (MAP) in mice. NRA0160 and clozapine antagonized MAP-induced stereotyped behavior in mice, although their effects did not exceed 50% inhibition, even at the highest dose given. NRA0160 and clozapine significantly induced catalepsy in rats, although their effects did not exceed 50% induction even at the highest dose given. NRA0160 and clozapine significantly reversed the disruption of prepulse inhibition (PPI) in rats produced by apomorphine. NRA0160 and clozapine significantly shortened the phencyclidine (PCP)-induced prolonged swimming latency in rats in a water maze task. These findings suggest that NRA0160 may have unique antipsychotic activities without the liability of motor side effects typical of classical antipsychotics. \n",
      "Entities: \n",
      "1. Entity1: ['dopamine'] (Chemical), Entity2: ['hyperactivity'] (Disease)\n",
      "2. Entity1: ['dopamine'] (Chemical), Entity2: ['catalepsy'] (Disease)\n",
      "3. Entity1: ['5 - [2- ( 4- ( 3 - fluorobenzylidene) piperidin-1-yl) ethyl] - 4 -(4-fluorophenyl) thiazole-2-carboxamide', 'NRA0160'] (Chemical), Entity2: ['hyperactivity'] (Disease)\n",
      "4. Entity1: ['5 - [2- ( 4- ( 3 - fluorobenzylidene) piperidin-1-yl) ethyl] - 4 -(4-fluorophenyl) thiazole-2-carboxamide', 'NRA0160'] (Chemical), Entity2: ['catalepsy'] (Disease)\n",
      "5. Entity1: ['5-HT', 'serotonin'] (Chemical), Entity2: ['hyperactivity'] (Disease)\n",
      "6. Entity1: ['5-HT', 'serotonin'] (Chemical), Entity2: ['catalepsy'] (Disease)\n",
      "7. Entity1: ['clozapine'] (Chemical), Entity2: ['hyperactivity'] (Disease)\n",
      "8. Entity1: ['clozapine'] (Chemical), Entity2: ['catalepsy'] (Disease)\n",
      "9. Entity1: ['MAP', 'methamphetamine'] (Chemical), Entity2: ['hyperactivity'] (Disease)\n",
      "10. Entity1: ['MAP', 'methamphetamine'] (Chemical), Entity2: ['catalepsy'] (Disease)\n",
      "11. Entity1: ['apomorphine'] (Chemical), Entity2: ['hyperactivity'] (Disease)\n",
      "12. Entity1: ['apomorphine'] (Chemical), Entity2: ['catalepsy'] (Disease)\n",
      "13. Entity1: ['PCP', 'phencyclidine'] (Chemical), Entity2: ['hyperactivity'] (Disease)\n",
      "14. Entity1: ['PCP', 'phencyclidine'] (Chemical), Entity2: ['catalepsy'] (Disease)\n",
      "\n",
      "Respond in the following format:\n",
      "Reasoning: [one sentence explaining why]\n",
      "Relation: [a numbered list of all entetiy pairs and their relations]\n",
      "\n",
      "You are a biomedical relation extraction expert.\n",
      "Prompt_ID:\n",
      "Task: Identify if the realtion between two specified entities is chemically inducing a disease or symptoms of a disease.\n",
      "\n",
      "Valid relation types (respond with EXACTLY ONE of these words):\n",
      "- CID: stands for Chemically induced Disease, give this label if the two enteties are a chemical and a disease where the chemical induces the disease or its symptoms. \n",
      "- No_Relation: The two enteties are according to the text not involved in chemically inducing a Disease.\n",
      "\n",
      "CRITICAL INSTRUCTIONS:\n",
      "1. Only annotate EXPLICIT relations stated in the text\n",
      "2. If entities co-occur WITHOUT a clear relationship  respond with \"No_Relation\"\n",
      "3. Do NOT infer or assume relationships not explicitly stated\n",
      "4. When uncertain  respond with \"No_Relation\"\n",
      "Example 1\n",
      "## Input:\n",
      "Text: Intraoperative bradycardia and hypotension associated with timolol and pilocarpine eye drops. A 69-yr-old man, who was concurrently being treated with pilocarpine nitrate and timolol maleate eye drops, developed a bradycardia and became hypotensive during halothane anaesthesia. Both timolol and pilocarpine were subsequently identified in a 24-h collection of urine. Timolol (but not pilocarpine) was detected in a sample of plasma removed during surgery; the plasma concentration of timolol (2.6 ng ml-1) was consistent with partial beta-adrenoceptor blockade. It is postulated that this action may have been enhanced during halothane anaesthesia with resultant bradycardia and hypotension. Pilocarpine may have had a contributory effect.\n",
      "Entity1: ['Timolol', 'timolol', 'timolol maleate'] (Chemical), Entity2: ['bradycardia'] (Disease)\n",
      "## Output:\n",
      "Entity1: ['Timolol', 'timolol', 'timolol maleate'] (Chemical), Entity2: ['bradycardia'] (Disease), Relation: CID\n",
      "\n",
      "Example 2\n",
      "## Input:\n",
      "Text: A pilot study to assess the safety of dobutamine stress echocardiography in the emergency department evaluation of cocaine-associated chest pain. STUDY OBJECTIVE: Chest pain in the setting of cocaine use poses a diagnostic dilemma. Dobutamine stress echocardiography (DSE) is a widely available and sensitive test for evaluating cardiac ischemia. Because of the theoretical concern regarding administration of dobutamine in the setting of cocaine use, we conducted a pilot study to assess the safety of DSE in emergency department patients with cocaine-associated chest pain. METHODS: A prospective case series was conducted in the intensive diagnostic and treatment unit in the ED of an urban tertiary-care teaching hospital. Patients were eligible for DSE if they had used cocaine within 24 hours preceding the onset of chest pain and had a normal ECG and tropinin I level. Patients exhibiting signs of continuing cocaine toxicity were excluded from the study. All patients were admitted to the hospital for serial testing after the DSE testing in the intensive diagnostic and treatment unit. RESULTS: Twenty-four patients were enrolled. Two patients had inadequate resting images, one DSE was terminated because of inferior hypokinesis, another DSE was terminated because of a rate-related atrial conduction deficit, and 1 patient did not reach the target heart rate. Thus, 19 patients completed a DSE and reached their target heart rates. None of the patients experienced signs of exaggerated adrenergic response, which was defined as a systolic blood pressure of greater than 200 mm Hg or the occurrence of tachydysrhythmias (excluding sinus tachycardia). Further suggesting lack of exaggerated adrenergic response, 13 (65%) of 20 patients required supplemental atropine to reach their target heart rates. CONCLUSION: No exaggerated adrenergic response was detected when dobutamine was administered to patients with cocaine-related chest pain.\n",
      "Entity1: ['atropine'] (Chemical), Entity2: ['sinus tachycardia'] (Disease)\n",
      "## Output:\n",
      "Entity1: ['atropine'] (Chemical), Entity2: ['sinus tachycardia'] (Disease), Relation: No_Relation\n",
      "\n",
      "---\n",
      "\n",
      "Analyze this case:\n",
      "Text: A selective dopamine D4 receptor antagonist, NRA0160: a preclinical neuropharmacological profile. NRA0160, 5 - [2- ( 4- ( 3 - fluorobenzylidene) piperidin-1-yl) ethyl] - 4 -(4-fluorophenyl) thiazole-2-carboxamide, has a high affinity for human cloned dopamine D4.2, D4.4 and D4.7 receptors, with Ki values of 0.5, 0.9 and 2.7 nM, respectively. NRA0160 is over 20,000fold more potent at the dopamine D4.2 receptor compared with the human cloned dopamine D2L receptor. NRA0160 has negligible affinity for the human cloned dopamine D3 receptor (Ki=39 nM), rat serotonin (5-HT)2A receptors (Ki=180 nM) and rat alpha1 adrenoceptor (Ki=237 nM). NRA0160 and clozapine antagonized locomotor hyperactivity induced by methamphetamine (MAP) in mice. NRA0160 and clozapine antagonized MAP-induced stereotyped behavior in mice, although their effects did not exceed 50% inhibition, even at the highest dose given. NRA0160 and clozapine significantly induced catalepsy in rats, although their effects did not exceed 50% induction even at the highest dose given. NRA0160 and clozapine significantly reversed the disruption of prepulse inhibition (PPI) in rats produced by apomorphine. NRA0160 and clozapine significantly shortened the phencyclidine (PCP)-induced prolonged swimming latency in rats in a water maze task. These findings suggest that NRA0160 may have unique antipsychotic activities without the liability of motor side effects typical of classical antipsychotics. \n",
      "Entity1: ['dopamine'] (Chemical), Entity2: ['hyperactivity'] (Disease)\n",
      "\n",
      "Respond in the following format:\n",
      "Reasoning: [Explanaiton why]\n",
      "Relation: [one word from the list above]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(calibration_test[\"n_1\"][\"run_1\"][\"multi_examples\"][0])\n",
    "print(calibration_test[\"n_1\"][\"run_1\"][\"single_examples\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
